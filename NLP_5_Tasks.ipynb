{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyMD/Wo9RKreGlKaeFizBwM0",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/123gamal/Python_Practice/blob/main/NLP_5_Tasks.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "***Task one : Multilingual Text Preprocessing Pipeline***\n"
      ],
      "metadata": {
        "id": "dywkC8Z9BgeU"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JpCIH_y4Bcaq",
        "outputId": "4598b351-68c3-4878-ae78-30b9875e0234"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['english', 'sentenc']\n",
            "['نص', 'لغة', 'عرب']\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ]
        }
      ],
      "source": [
        "import nltk\n",
        "import re\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import SnowballStemmer, ISRIStemmer\n",
        "\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')\n",
        "\n",
        "def preprocess_text(text, lang='english'):\n",
        "    text = re.sub(r'[^\\w\\s]', '', text.lower())\n",
        "    if lang == 'english':\n",
        "        tokens = nltk.word_tokenize(text, language='english')\n",
        "        stop_words = set(stopwords.words('english'))\n",
        "        stemmer = SnowballStemmer(\"english\")\n",
        "    elif lang == 'arabic':\n",
        "        tokens = nltk.word_tokenize(text)\n",
        "        stop_words = set(stopwords.words('arabic'))\n",
        "        stemmer = ISRIStemmer()\n",
        "    else:\n",
        "        raise ValueError(\"Unsupported language\")\n",
        "\n",
        "    filtered = [stemmer.stem(w) for w in tokens if w not in stop_words]\n",
        "    return filtered\n",
        "\n",
        "print(preprocess_text(\"This is an English sentence.\"))\n",
        "print(preprocess_text(\"هٰذا نَصٌّ بِاللُّغَةِ العَرَبِيَّةِ\", lang='arabic'))"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "***Task 2:Based Word Similarity***\n",
        "\n"
      ],
      "metadata": {
        "id": "TYqdsx3KB4DB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import fasttext\n",
        "import fasttext.util\n",
        "import gzip\n",
        "import shutil\n",
        "import os\n",
        "\n",
        "# Download Arabic fastText model (only need to do once)\n",
        "# Check if the uncompressed file already exists\n",
        "if not os.path.exists(\"cc.ar.300.bin\"):\n",
        "    # Download the compressed file\n",
        "    if not os.path.exists(\"cc.ar.300.bin.gz\"):\n",
        "        !wget https://dl.fbaipublicfiles.com/fasttext/vectors-crawl/cc.ar.300.bin.gz\n",
        "\n",
        "    # Decompress the file\n",
        "    with gzip.open(\"cc.ar.300.bin.gz\", 'rb') as f_in:\n",
        "        with open(\"cc.ar.300.bin\", 'wb') as f_out:\n",
        "            shutil.copyfileobj(f_in, f_out)\n",
        "    print(\"Decompression complete.\")\n",
        "else:\n",
        "    print(\"Uncompressed model file already exists.\")\n",
        "\n",
        "\n",
        "# Load model from the uncompressed file\n",
        "ft = fasttext.load_model(\"cc.ar.300.bin\")\n",
        "\n",
        "# Similarity between words\n",
        "vec1 = ft.get_word_vector(\"الذكاء\")\n",
        "vec2 = ft.get_word_vector(\"الاصطناعي\")\n",
        "\n",
        "# Cosine similarity\n",
        "import numpy as np\n",
        "cos_sim = np.dot(vec1, vec2) / (np.linalg.norm(vec1) * np.linalg.norm(vec2))\n",
        "print(\"Similarity between 'الذكاء' and 'الاصطناعي':\", cos_sim)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2ikWASgRB6LM",
        "outputId": "d6c7814a-8176-4585-9313-fda2c83f0e9a"
      },
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Decompression complete.\n",
            "Similarity between 'الذكاء' and 'الاصطناعي': 0.43998143\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "***Task 3: Build a Q&A Chatbot Using Transformers***"
      ],
      "metadata": {
        "id": "_8tYBXrnCC8a"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import pipeline\n",
        "\n",
        "qa = pipeline(\"question-answering\", model=\"aubmindlab/bert-base-arabertv2\")\n",
        "\n",
        "context = \"\"\"\n",
        "اللغة العربية هي إحدى أكثر اللغات انتشاراً في العالم، يتحدث بها أكثر من 400 مليون نسمة في الوطن العربي ومناطق أخرى.\n",
        "تُعدّ من اللغات الرسمية في منظمة الأمم المتحدة.\n",
        "\"\"\"\n",
        "\n",
        "question = \"كم عدد المتحدثين باللغة العربية؟\"\n",
        "\n",
        "result = qa(question=question, context=context)\n",
        "print(f\"Answer: {result['answer']}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "N914bs5DCAjp",
        "outputId": "557d729d-a8bd-49cb-fa91-3503f2f4be89"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of BertForQuestionAnswering were not initialized from the model checkpoint at aubmindlab/bert-base-arabertv2 and are newly initialized: ['qa_outputs.bias', 'qa_outputs.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Device set to use cuda:0\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Answer: بها أكثر من 400 مليون نسمة في الوطن العربي\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "***Task 4: Topic Modeling with LDA on Arabic News***"
      ],
      "metadata": {
        "id": "4FX74qyECQB6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from gensim import corpora, models\n",
        "from nltk.tokenize import word_tokenize\n",
        "import re\n",
        "from nltk.corpus import stopwords\n",
        "\n",
        "\n",
        "def preprocess_arabic(text):\n",
        "    text = re.sub(r'[^\\w\\s]', '', text)\n",
        "    tokens = word_tokenize(text)\n",
        "    arabic_stop_words = set(stopwords.words('arabic'))\n",
        "    return [word for word in tokens if word not in arabic_stop_words and len(word) > 2]\n",
        "\n",
        "documents = [\n",
        "    \"السياسة في الشرق الأوسط معقدة ومتشابكة.\",\n",
        "    \"يعاني الاقتصاد العالمي من التضخم.\",\n",
        "    \"تتطور تقنيات الذكاء الاصطناعي بسرعة.\",\n",
        "    \"الرياضة تساعد على تحسين الصحة العامة.\",\n",
        "]\n",
        "\n",
        "processed_docs = [preprocess_arabic(doc) for doc in documents]\n",
        "dictionary = corpora.Dictionary(processed_docs)\n",
        "corpus = [dictionary.doc2bow(text) for text in processed_docs]\n",
        "\n",
        "lda = models.LdaModel(corpus, num_topics=2, id2word=dictionary, passes=10)\n",
        "topics = lda.print_topics()\n",
        "for topic in topics:\n",
        "    print(topic)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "T4QT-97JCLjD",
        "outputId": "273f457a-8bd5-482b-9f01-2603686fecea"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: gensim in /usr/local/lib/python3.11/dist-packages (4.3.3)\n",
            "Requirement already satisfied: numpy<2.0,>=1.18.5 in /usr/local/lib/python3.11/dist-packages (from gensim) (1.26.4)\n",
            "Requirement already satisfied: scipy<1.14.0,>=1.7.0 in /usr/local/lib/python3.11/dist-packages (from gensim) (1.13.1)\n",
            "Requirement already satisfied: smart-open>=1.8.1 in /usr/local/lib/python3.11/dist-packages (from gensim) (7.3.0.post1)\n",
            "Requirement already satisfied: wrapt in /usr/local/lib/python3.11/dist-packages (from smart-open>=1.8.1->gensim) (1.17.3)\n",
            "(0, '0.110*\"التضخم\" + 0.110*\"يعاني\" + 0.110*\"الاقتصاد\" + 0.110*\"العالمي\" + 0.038*\"الأوسط\" + 0.038*\"السياسة\" + 0.038*\"معقدة\" + 0.038*\"بسرعة\" + 0.037*\"تقنيات\" + 0.037*\"تتطور\"')\n",
            "(1, '0.061*\"تساعد\" + 0.061*\"الصحة\" + 0.061*\"تحسين\" + 0.061*\"العامة\" + 0.061*\"الرياضة\" + 0.061*\"الاصطناعي\" + 0.061*\"الشرق\" + 0.061*\"الذكاء\" + 0.061*\"ومتشابكة\" + 0.061*\"تتطور\"')\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "***Task 5: Build a Custom Tokenizer for Arabic Diacritized Text***"
      ],
      "metadata": {
        "id": "j69KueMeCVZf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "\n",
        "arabic_diacritics = re.compile(r'[\\u0617-\\u061A\\u064B-\\u0652]')\n",
        "\n",
        "def remove_diacritics(text):\n",
        "    return re.sub(arabic_diacritics, '', text)\n",
        "\n",
        "def custom_tokenize(text):\n",
        "    text = remove_diacritics(text)\n",
        "    text = re.sub(r'[^\\w\\s]', '', text)\n",
        "    text = re.sub(r'[ـ]+', '', text)\n",
        "    tokens = text.split()\n",
        "    return tokens\n",
        "\n",
        "text = \"اللُّغَةُ العَرَبِيَّةُ جَمِيلَةٌ\"\n",
        "tokens = custom_tokenize(text)\n",
        "print(tokens)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2x-CXnBpCZkD",
        "outputId": "7b125580-83f5-42d3-8ef8-b5b710338b5e"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['اللغة', 'العربية', 'جميلة']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9dfd017e",
        "outputId": "1271e88d-5ccf-4a45-ed4a-99c037a08031"
      },
      "source": [
        "!pip install pyarabic"
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: pyarabic in /usr/local/lib/python3.11/dist-packages (0.6.15)\n",
            "Requirement already satisfied: six>=1.14.0 in /usr/local/lib/python3.11/dist-packages (from pyarabic) (1.17.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b0c5051c",
        "outputId": "7e8ebc53-3aba-4bdd-f556-9dd973706220"
      },
      "source": [
        "!pip install farasa"
      ],
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting farasa\n",
            "  Downloading Farasa-0.0.1-py2.py3-none-any.whl.metadata (1.4 kB)\n",
            "Downloading Farasa-0.0.1-py2.py3-none-any.whl (12.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m12.6/12.6 MB\u001b[0m \u001b[31m81.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: farasa\n",
            "Successfully installed farasa-0.0.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c0a9c051",
        "outputId": "af36bbc6-20df-44df-cb2f-6d95b8b39b6a"
      },
      "source": [
        "!pip install fasttext"
      ],
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting fasttext\n",
            "  Downloading fasttext-0.9.3.tar.gz (73 kB)\n",
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/73.4 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m73.4/73.4 kB\u001b[0m \u001b[31m2.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting pybind11>=2.2 (from fasttext)\n",
            "  Using cached pybind11-3.0.0-py3-none-any.whl.metadata (10.0 kB)\n",
            "Requirement already satisfied: setuptools>=0.7.0 in /usr/local/lib/python3.11/dist-packages (from fasttext) (75.2.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from fasttext) (1.26.4)\n",
            "Using cached pybind11-3.0.0-py3-none-any.whl (292 kB)\n",
            "Building wheels for collected packages: fasttext\n",
            "  Building wheel for fasttext (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for fasttext: filename=fasttext-0.9.3-cp311-cp311-linux_x86_64.whl size=4508439 sha256=53459919ca12db42ee2a16cc8228d54371d41240e275c746b76e72726b3ad939\n",
            "  Stored in directory: /root/.cache/pip/wheels/65/4f/35/5057db0249224e9ab55a513fa6b79451473ceb7713017823c3\n",
            "Successfully built fasttext\n",
            "Installing collected packages: pybind11, fasttext\n",
            "Successfully installed fasttext-0.9.3 pybind11-3.0.0\n"
          ]
        }
      ]
    }
  ]
}